{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is probability?\n",
    "\n",
    "__Frequentist interpretation__ - probabilities represent long run frequencies of events\n",
    "\n",
    "__Bayesian interpretation__ - probability is used to represent a degree of belief about an uncertain event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability axioms\n",
    "\n",
    "1. __Nonnegativity__: $\\mathbf{P}(A)\\geq0$, for every event $A$\n",
    "2. __Additivity__: $\\mathbf{P}(A\\cup B)=\\mathbf{P}(A)+\\mathbf{P}(B)$, if $A$ and $B$ are disjoint\n",
    "3. __Normalization__: $\\mathbf{P}(\\Omega)=1$, where $\\Omega$ is the entire sample space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random variables\n",
    "\n",
    "-  A __random variable__ is a real-valued function of the experiment outcome.\n",
    "-  A __function of a random variable__ defines another random variable\n",
    "-  We can associate with each random variable certain \"averages\" of interest, such as the __mean__ and the __variance__\n",
    "-  A random variable can be __conditioned__ on an event or on another random variable\n",
    "-  There is a notion of __independence__ of a random variable from and event or from another random variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discrete random variables\n",
    "\n",
    "-  A __discrete random variable__ is a real-valued function of the outcome the experiment that can take a finite or countable infinite number of values\n",
    "-  A discrete random variable has an associated __probability mass function (PMF)__, which gives the probability of each numerical value that the random variable can take\n",
    "-  A __function of a discrete random variable__ defines another discrete random variable, whose PMF can be obtained from the PMF of the original random variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Probability mass function\n",
    "As per probability axioms PMF must satisfy the following properties\n",
    "\n",
    "1. $\\,\\,\\forall{x}\\in{X}\\,,0\\leq P(X=x)\\leq1$\n",
    "\n",
    "2. $\\,\\,\\sum_{x\\in{X}}P(X=x)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Continuous random variables\n",
    "\n",
    "A random variable $X$ is called __continuous__ if there is a nonnegative function $p$ called the __probability density__ function of $X$, such that:\n",
    "\n",
    "$\\mathbf{P}(X\\in{B})=\\int_B{p(x)dx}\\,\\,\\,$, for every subset $B$ of the real line. \n",
    "\n",
    "In particular,\n",
    "\n",
    "$\\mathbf{P}(a\\leq X\\leq b) = \\int_a^b{p(x)dx}$\n",
    "\n",
    "$\\mathbf{P}(-\\infty\\leq X\\leq\\infty) = \\int_{-\\infty}^{\\infty}{p(x)dx}=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginal probability\n",
    "\n",
    "Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. This is called __marginal probability distribution__ and we can calculate it with the __sum rule__.\n",
    "\n",
    "$\\forall_{x\\in{X}},\\,P(X=x)=\\sum_y P(X=x,Y=y)$\n",
    "\n",
    "$p(x)=\\int{p(x,y)dy}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional probability\n",
    "\n",
    "\n",
    "We are interested in the probability of some event, given that some other event has happend. This is called a __conditional probability__.\n",
    "\n",
    "\n",
    "$P(Y=y|X=x)=\\dfrac{P(Y=y,X=x)}{P(X=x)}$\n",
    "\n",
    "The __chain rule of conditional probabilities__\n",
    "\n",
    "$P(X^{(1)},...,X^{(n)})=P(X^{(1)})\\prod_{i=2}^n P(X^{(i)}|X^{(1)},...,X^{(i-1)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence and Conditional Independence\n",
    "\n",
    "Two random variables $X$ and $Y$ are __independent__ if:\n",
    "\n",
    "$\\forall_{x\\in{X},y\\in{Y}},\\,P(X=x,Y=y)=P(X=x)P(Y=y)$\n",
    "\n",
    "Twor random variables $X$ and $Y$ are __conditionally independent__ given a random variable $Z$ if\n",
    "\n",
    "$\\forall_{x\\in{X},y\\in{Y},z\\in{Z}},\\,P(X=x,Y=y|Z=z)=P(X=x|Z=z)P(Y=y|Z=z)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "Combining the defintion of conditional probability with the product and sum rules yields __Bayes rule__.\n",
    "\n",
    "\n",
    "$P(x\\,|y)=\\dfrac{P(x,\\,y)}{P(y)}=\\dfrac{P(x)P(y\\,|x)}{P(y)}=\\dfrac{P(x)P(y\\,|x)}{\\sum_{x\\in{X}}P(x)P(y\\,|x)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Expectations\n",
    "\n",
    "The __expectation__ or expected value of some function $f(x)$ with respect to a probability distribution $P(X)$ is the average, or mean value, that $f$ takes on when $x$ is drawn from $P$.\n",
    "\n",
    "For discrete random variables:\n",
    "\n",
    "$\\mathbb{E}_{X\\sim P}[f(x)]=\\sum_x{P(x)f(x)}$\n",
    "\n",
    "For continuous random variables:\n",
    "\n",
    "$\\mathbb{E}_{X\\sim p}[f(x)]=\\int{p(x)f(x)dx}$\n",
    "\n",
    "Expectations are linear:\n",
    "\n",
    "$\\mathbb{E}_X[\\alpha f(x)+\\beta g(x)]=\\alpha \\mathbb{E}_X[f(x)] + \\beta \\mathbb{E}_X[g(x)]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance and Covariance \n",
    "\n",
    "The __variance__ gives a measure of how the values of a function of a random variable $X$ vary as we sample difference values $x$ from its probability distribution:\n",
    "\n",
    "$\\mathsf{Var}(f(x))=\\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])^2]$\n",
    "\n",
    "The __covariance__ gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:\n",
    "\n",
    "$\\mathsf{Cov}(f(x),\\,g(y))=\\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])(g(y)-\\mathbb{E}[g(y)])]$\n",
    "\n",
    "The __covariance matrix__ of a random vector $\\mathbf{x}\\in \\mathbb{R}^n$, is an $n\\,\\mathsf{x}\\,n$ matrix, such that:\n",
    "\n",
    "$\\mathsf{Cov}(\\mathbf{x})_{i,j}=\\mathsf{Cov}(x_i, x_j)$\n",
    "\n",
    "The diagonal elements of the convariance matrix give the variance.\n",
    "\n",
    "$\\mathsf{Cov}(x_i,x_i)=\\mathsf{Var}(x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
