{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Primer\n",
    "\n",
    "__Sources:__\n",
    "\n",
    "-  Kevin P. Murphy (2012). Machine Learning. A Probabilistic Perspective.\n",
    "-  Christopher Bishop (2006). Pattern Recognition and Machine Learning.\n",
    "-  Yaser Abu-Mostafa et al. (2012). Learning From Data.\n",
    "-  Trevor Hastie et al. (2008). The Elements of Statistical Learning.\n",
    "-  Tom M. Mitchell (1997). Machine Learning\n",
    "-  Ian Goodfellow, Yoshoua Bengio, Aaron Courville (2016). Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is machine learning?\n",
    "\n",
    "*\"Machine learning is a set of methods that can automatically detect patterns in data, and then use the uncovered patterns to predict future data, or peform other kinds of decision making under uncertainty\"* __(Kevin P. Murphy).__\n",
    "\n",
    "*\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E\"* __(Tom M. Mitchell).__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of machine learning\n",
    "-  Supervised learning\n",
    "-  Unsupervised learning\n",
    "-  Semi-supervised learning\n",
    "-  Reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised learning\n",
    "\n",
    "Learning a mapping from inputs $\\mathbf{x}$ to outputs $\\mathbf{y}$, given a labeled set of input-output pairs $\\mathcal{D}=\\{(\\mathbf{x_i},\\mathbf{y_i})\\}_{i=1}^N$.\n",
    "\n",
    "$\\mathcal{D}$ is called the __training set__, where $N$ is the number of training examples.\n",
    "\n",
    "In the simplest setting, each $\\mathbf{x_i}$ is a vector of numbers. In general, $\\mathbf{x_i}$ could be a complex structured object, such as an image or sentence. Components of $\\mathbf{x_i}$ are referred to as __features__, __attributes__, or __covariates__.\n",
    "\n",
    "The output $\\mathbf{y_i}$, often referred to as response variable, can in principle be anything, but most methods assume that $\\mathbf{y_i}$ is a __categorical__ or __nominal__ variable from some finite set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised learning as function approximation\n",
    "\n",
    "From one point of view, supervised learning can be considered  __function approximation__.\n",
    "\n",
    "Let's assume $y=f(x)$ to be some unknown function that generates our training data. The goal of learning is to estimate the function $f$ given a labeled training set generated by this function, and then to make predictions on __new (outside of training set)__ using $\\hat{y}=\\hat{f}(\\mathbf{x})$\n",
    "\n",
    "\n",
    "This is called __generalization__.\n",
    "\n",
    "The training data set may be noisy:\n",
    "-  corrupted during collection\n",
    "-  not all attributes used by $f(x)$ may be observerd - unobservibility problem\n",
    "\n",
    "The data generation process may also be inherently stochastic.\n",
    "\n",
    "As such probabilistic techniques are very important in all forms of machine learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Probabilistic view of supervised learning\n",
    "\n",
    "We can model a supervised learning problem as a conditional probability $p(y|\\mathbf{x},\\mathcal{D})$ on the __test__ input $\\mathbf{x}$ and the training set $\\mathcal{D}$. Given a probabilistic output, we can make predictions using:\n",
    "\n",
    "$$\\hat{y}=\\hat{f}(\\mathbf{x})=\\underset{c\\in{C}}{\\arg\\max}\\, {p}(y=c|\\mathbf{x},\\mathcal{D})$$\n",
    "\n",
    "This is called a __MAP estimate__ (MAP stands for maximum a posteriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification\n",
    "\n",
    "Learn a mapping from inputs $\\mathbf{x}$ to outputs $y$, where\n",
    "$y\\in\\{1,...,C\\}$, with $C$ being the number of classes.\n",
    "\n",
    "If $C=2$, this is called __binary classification__ and it oftend assumed that $y\\in\\{0,1\\}$\n",
    "\n",
    "If $C>2$, this is called __multiclass classification__.\n",
    "\n",
    "If the class labels are not mutually exclusive this is called __multi-label classification__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression\n",
    "\n",
    "In regression, $y_i\\in{\\mathbb{R}}$\n",
    "\n",
    "Examples:\n",
    "-  Stock price predictions\n",
    "-  RUL (Remaining Usefull Life) of of a piece of equimpment in IoT scenarios\n",
    "-  The temperature at any location inside a building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "Given inputs, $\\mathcal{D}=\\{\\mathbf{x_i}\\}_{i=1}^N$, find \"patterns\", \"structure\", or other insights in data.\n",
    "\n",
    "A probabilistic approach to formalize unsuprevised learning is __denstity estimation__. That is, building models of the form $p(\\mathbf{x_i}|\\Theta)$, where $\\Theta$ are parameters of a considered parametric model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discovering clusters\n",
    "\n",
    "Let $z_i\\in{\\{1,...,K\\}}$ represetn the cluster to which data point $i$ is assigned. ($z_i$ i a __latent__ variable, since it is never observed in the training set). The clustering task can be defined as:\n",
    "$$z_j^*=\\underset{k}{\\arg\\max}\\,p(z_j=k|\\mathbf{x_i},\\mathcal{D})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discoverying latent factors\n",
    "\n",
    "A.k.a __dimensionality reduction__. The data may appear high dimensional, but there may be a small number of degrees of variability, corresponding to __latent factors__.\n",
    "\n",
    "The common approach to dimensionality reduction is __principal components analysis__ or __PCA__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discovering graph structure\n",
    "\n",
    "Sometimes we observe a set of correlated variables, and we would like to discover which ones are most correlated with which others. This can be represented by a graph $G$, in which nodes represent variables, and edges represent direct dependence between variables.\n",
    "\n",
    "This task can be defined as:\n",
    "\n",
    "$$\\hat{G}={\\arg\\max}\\,p(G|\\mathcal{D})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Collaborative filtering\n",
    "\n",
    "E.g. predicting which movies people will want to watch based on how they, and other people. have rated movies which they have already seen.\n",
    "\n",
    "We learn a matrix $\\mathbf{X}$ where $X(m,u)$ is the rating by user $u$ of movie $m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market basket analysis\n",
    "\n",
    "The data consists of a (very large but sparse) binary matrix, where each column represents an item or product, and each row represent a transaction. E.g. $x_{ij}=1$ if the item $j$ was purchased by the $i$'th transaction.\n",
    "\n",
    "Given a new partially observed bit vector, representing a subset of items that the consumer has put in the basket, the goal is to predict which other items the consumer might be likely to buy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "\n",
    "Learn how to act or behave when given occasional reward or punishment signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic concepts in machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalization\n",
    "\n",
    "The central challenge in machine learning is that the algorithm must perform well on _new, previously unseen inputs_ - not just those on which the model was trained.\n",
    "\n",
    "This is called __generalization__.\n",
    "\n",
    "The measure of generalization - __the generalization error__, also called the __test error__ - is typically estimated on a __test set__ of examples that were collected separatly from the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Futility of bias free learning\n",
    "\n",
    "A learner that makes no a priori assumptions about the data distribution that generated training examples has no rational basis for predicting any unseen instances.\n",
    "\n",
    "Inductive learning requires some form of prior assumptions, or __inductive bias__.\n",
    "\n",
    "Inductive reasoning, without inductive bias, is not logically valid.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### No free lunch theorem (Wolpert 1996)\n",
    "\n",
    "There is no universal learner.\n",
    "\n",
    "No learner can succeed on all learning tasks.\n",
    "\n",
    "Averaged over all possible data-generating distributions, every classification algorithm has the same error rate when classifying previously unobserved examples. The most sophisticated algorithm has the same average performance (__over all possible tasks__) as merely predicting that every point belongs to the same class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model selection\n",
    "\n",
    "As a consequence of the no free lunch theorem, you may need to develop and compare many different models to cover the wide variery of data distributions that occur in the real world.\n",
    "\n",
    "A set of assumptions that works well in one domain (and preferentiate one model) may work poorly in another (and for another model).\n",
    "\n",
    "This is usually an experimental process striving to find the model with the lowest __generalization error__ for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model capacity\n",
    "\n",
    "Informally, a model's capacity is its ability to fit a wide variety of functions. \n",
    "\n",
    "\n",
    "Statistical learning theory provides ways of formally quantifying model capacity. The most well known one is the __Vapnik-Chervonenkis (VC) dimension__. \n",
    "\n",
    "The VC dimension measures the capacity of a binary classifier The VC dimension is defined as being the largest possible value of $m$ for which there exists a training set of $m$ different $\\mathbf{x}$ points that the classifier can label aribitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting and underfitting\n",
    "\n",
    "Models with low capacity may struggle to fit the training set. This is called underfitting.\n",
    "\n",
    "Highly flexible (high capacity) models can easily overfit data. That is the model can fit every minor variation in the input including noise and \"phantoms\".\n",
    "\n",
    "Both phenomena hurt __generalization__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())r\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parametric vs non-parametric models\n",
    "\n",
    "A __parametric model__ has a fixed number of parameters. \n",
    "\n",
    "In a __non-parametric model__ the number of parameters grows with the amount of training data. \n",
    "\n",
    "Parametric models make stronger assumptions about the nature of the data distribution.\n",
    "\n",
    "Non-parametric models are more flexible, but often computationally intractable for large datasets and tend to suffer from the curse of dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The curse of dimensionality\n",
    "\n",
    "High dimensional datasets tend to be very sparse. Meaning that high dimensional spaces are \"very barren\". The concept of \"nearest neighbor\" quickly looses meaning as a number of dimensions increases. \n",
    "\n",
    "The models (like most non-parametric models) that rely on similarity and distance measures very quickly degenerate in performance as \"everybody is far away\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimators\n",
    "\n",
    "Point estimation attempts to provide the single \"best\" prediction of some quantity of interest. In the context of machine learning, the quantity of interest is usually a set of parameters in some parametric model, but it can also be a whole function.\n",
    "\n",
    "A __point estimator__ or __statistic__ is any function of the data:\n",
    "\n",
    "$\\hat{\\mathbf{\\Theta}}=g(\\mathbf{x^{(1)}},...,\\mathbf{x^{(m)}}), \\,\\,$ where $\\{\\mathbf{x^{(m)}},...\\mathbf{x^{(m)}}\\}$ is a set of $m$ independent and indetically distributed (i.i.d) data points.\n",
    "\n",
    "The bias of an estimator is defined as:\n",
    "\n",
    "$\\mathsf{bias}(\\hat{\\mathbf{\\Theta}})=\\mathbb{E}[\\hat{\\mathbf{\\Theta}}]-\\mathbf{\\Theta}$\n",
    "\n",
    "An estimator is said to be __unbiased__ if $\\mathsf{bias}(\\hat{\\mathbf{\\Theta}})=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "Let $\\mathcal{D}=\\{\\mathbf{x^{(1)}},...,\\mathbf{x^{(m)}}\\}$ be a set i.i.d. examples drawn from the true by unknown data generating distribution $p_{data}(\\mathbf{x})$. Let $p_{model}(\\mathbf{x};\\mathbf{\\Theta})$ be a parametric family of probability distributions indexed by $\\mathbf{\\Theta}$. Let's also assume that the true distribution $p_{data}$ lies within the model family $p_{model}(;\\mathbf{\\Theta})$ and that the true distribution must correspond to exactly one value of $\\mathbf{\\Theta}$.\n",
    "\n",
    "The maximum likelihood estimator for $\\mathbf{\\Theta}$ is then defined as:\n",
    "\n",
    "$\\mathbf{\\Theta}_{ML} = \\underset{\\mathbf{\\Theta}}{\\arg\\max}\\, p_{model}(\\mathcal{D};\\mathbf{\\Theta})=\\underset{\\mathbf{\\Theta}}{\\arg\\max}\\,\\prod_{i=1}^m p_{model}(\\mathbf{x^{(i)}};\\mathbf{\\Theta})$\n",
    "\n",
    "To obtain a more convenient (e.g. better numerical stability) but equivalent optimization problem, we usually optimize the logarithm of the likelihood.\n",
    "\n",
    "$\\mathbf{\\Theta}_{ML} =\\underset{\\mathbf{\\Theta}}{\\arg\\max}\\,\\sum_{i=1}^m \\log\\,p_{model}(\\mathbf{x^{(i)}};\\mathbf{\\Theta})$\n",
    "\n",
    "By dividing by $m$ we can obtain a version of the criterion that is expressed as an expectation with respect to the empirical data distribution $\\hat{p}_{data}$ defined by the training data:\n",
    "\n",
    "$\\mathbf{\\Theta}_{ML} =\\underset{\\mathbf{\\Theta}}{\\arg\\max}\\,\\mathbb{E}_{\\mathbb{x}\\sim \\hat{p}_{data}} [\\log\\,p_{model}(\\mathbf{x^{(i)}};\\mathbf{\\Theta})]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KL divergence\n",
    "\n",
    "One way to interpret maximum likelihood estimation is to view it as minimizing the dissimilarity between the empirical distribution $\\hat p_{data}$, defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence. The KL divergence is given by:\n",
    "\n",
    "$D_{KL}(\\hat{p}_{data}\\Vert p_{model})=\\mathbb{E}_{\\mathbf{x}\\sim\\hat{p}_{data}}[\\log \\hat{p}_{data}(\\mathbf{x})-\\log p_{model}(\\mathbf{x})]$\n",
    "\n",
    "Since the term $\\log\\hat{p}_{data}(\\mathbf{x})$ is a function only of the data-generating process, not the model, then to minimize KL divergence with respect to the model we only need to minimize:\n",
    "\n",
    "$-\\mathbb{E}_{\\mathbf{x}\\sim\\hat{p}_{data}}[\\log p_{model}(\\mathbf{x})]$\n",
    "\n",
    "The above term is called the __cross-entropy__\n",
    "\n",
    "$H(\\hat{p}_{data}, p_{model})=-\\mathbb{E}_{\\mathbf{x}\\sim\\hat{p}_{data}}[\\log p_{model}(\\mathbf{x})]$\n",
    "\n",
    "The __cross-entropy__ is one of the most common loss function in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional Log-Likelihood\n",
    "\n",
    "The maximum likelihood estimation can by generalized to estimate a conditional probability  $P(\\mathbf{Y}\\,|\\,\\mathbf{X}; \\mathbf{\\Theta})$. This forms the basis for most supervised learning. If $\\mathbf{X}$ represents all inputs and $\\mathbf{Y}$ all observed targets, then the conditional maximum likelihood estimator is:\n",
    "\n",
    "$\\mathbf{\\Theta}_{ML}=\\underset{\\mathbf{\\Theta}}{\\arg\\max}\\,P(\\mathbf{Y}\\,|\\,\\mathbf{X};\\mathbf{\\Theta})$\n",
    "\n",
    "If observations are i.i.d this can be decomposed into:\n",
    "\n",
    "$\\mathbf{\\Theta}_{ML}=\\underset{\\mathbf{\\Theta}}{\\arg\\max}\\,\\sum_{i=1}^{m}\\log P(\\mathbf{y}^{(i)}\\,|\\,\\mathbf{x}^{(i)};\\mathbf{\\Theta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Bayesian Inference\n",
    "\n",
    "The frequentist perspective is that the true parameter $\\mathbf{\\Theta}$ is fixed and unknown, while the point estimate $\\hat\\mathbf{\\Theta}$ is a random variable on account of being a function of the dataset.\n",
    "\n",
    "The Bayesian perspective is that the true parameter $\\mathbf{\\Theta}$ is unknown or uncertain and thus represented as a random variable.\n",
    "\n",
    "Before observing the data, we represent out knowledgy of $\\mathbf{\\Theta}$ using the __prior probability distribution__ $p(\\mathbf{\\Theta})$, which is usually quite broad (i.e. with high entropy) to reflect a high degreee of uncertainty about $\\mathbf{\\Theta}$ before observing any data. \n",
    "\n",
    "After observing a set of data samples $\\{\\mathbf{x}_{(1)},...,\\mathbf{x}_{(m)}\\}$ we can update our degree of belief about $\\mathbf{\\Theta}$ by combining data likelihood with the prior using Bayes' rule:\n",
    "\n",
    "$p(\\mathbf{\\Theta}\\,|\\,\\mathbf{x}_{(1)},...,\\mathbf{x}_{(m)})=\\dfrac{p(\\mathbf{x}_{(1)},...,\\mathbf{x}_{(m)}\\,|\\,\\mathbf{\\Theta})p(\\mathbf{\\Theta})}{p(\\mathbf{x}_{(1)},...,\\mathbf{x}_{(m)})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian inference\n",
    "\n",
    "To make predictions we are using a full distribution over $\\mathbf{\\Theta}$. For example after observing $m$ examples, the predicted distribution over the next data sampel is given by:\n",
    "\n",
    "$p(\\mathbf{x}_{(m+1)}\\,|\\,\\mathbf{x}_{(1)},...,\\mathbf{x}_{(m)})=\\int p(\\mathbf{x}_{(m+1)}\\,|\\,\\mathbf{\\Theta})p(\\mathbf{\\Theta}\\,|\\,\\mathbf{x}_{(1)},...,\\mathbf{x}_{(m)})d\\mathbf{\\Theta}$\n",
    "\n",
    "Here each value of $\\mathbf{\\Theta}$ with positive density contributes to the prediction of the next example,with the contribution weighted by the posterior density itself.\n",
    "\n",
    "Also the prior has an influence by shifting probability mass density towards regions of the parameter space that are prefered a priori. In practice, the prior often expresses a preference for models that are simpler or more smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "While the most principle approach is to make predictions using the full Bayesian posterior distribution over $\\mathbf{\\Theta}$ it is in most cases (especially when dealing with large data sets) computationally prohibitive or intractable. \n",
    "\n",
    "Rather than simply returning to the maximuml likelihood estimation, we can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate. This approach is called __Maximum A Posteriori__ (MAP) point estimate.\n",
    "\n",
    "$\\mathbf{\\Theta}_{MAP} = \\underset{\\mathbf{\\Theta}}{\\arg\\max}\\, p(\\mathbf{\\Theta})\\,|\\,\\mathcal{D})=\\underset{\\mathbf{\\Theta}}{\\arg\\max}\\, p(\\mathcal{D}\\,|\\,\\mathbf{\\Theta})p(\\mathbf{\\Theta})=\\underset{\\mathbf{\\Theta}}{\\arg\\max}\\,\\log p(\\mathcal{D}\\,|\\,\\mathbf{\\Theta})+\\log p(\\mathbf{\\Theta})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
