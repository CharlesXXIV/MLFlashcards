{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Convolutional neural networks (CNNs) are a specialized neural networks for processing data that has a grid-like topology.\n",
    "\n",
    "CNNs have been very successful in image processing and computer vision applications.\n",
    "\n",
    "Convolutional neural networks utilize a specialized type of linear operation called __convolution__.\n",
    "\n",
    "You could define convolutional neural networks as neural networks that use convolution in place of general matrix multiplication in at least one of their layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The convolution operation\n",
    "\n",
    "In its most general form, convolution is an operation on two functions of a real-valued argument.\n",
    "\n",
    "Let $x(t)$ and $w(t)$ be two functions of $t\\in\\mathbb R$. The __convolution__  operation is defined as:\n",
    "\n",
    "$s(t)=(x \\ast w)(t)= \\int_{-\\infty}^{\\infty} x(a)w(t-a)da$\n",
    "\n",
    "$x$ is often referred to as the __input__ and the second argument $w$ as the __kernel__.\n",
    "\n",
    "If we assume that $x$ and $w$ are only defined on integers $t\\in \\mathbb I$, we can define the __discrete convolution__.\n",
    "\n",
    "$s(t)=(x \\ast w)(t)=\\sum_{a=-\\infty}^{\\infty} x(a)w(t-a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The convolution operation on tensors\n",
    "\n",
    "In machine learning, the inputs are usually a tensor of data, and the kernel a tensor of parameters. Since both tensors are finite we evolve the general form of convolution to substitue the infinite summation with a summation over a finite number of tensor elements - it is assumed that the values are zero everywhere but in the finite set of points stored by tensors.\n",
    "\n",
    "We can define a convolution operation on tensors as follows:\n",
    "\n",
    "Let $\\mathbf X$ and $\\mathbf W$ be two-dimensional matrices (rank 2 tensors) .The convolution operation yields another two-dimensional matrix whose elements are defined as:\n",
    "\n",
    "$z_{i,j}=(\\mathbf X \\ast \\mathbf W)_{i,j}=\\sum_m\\sum_n x_{m,n} w_{i-m,j-n}$. \n",
    "\n",
    "Notice that as $m$ and $n$ increase, the index into the input increases, but the index into the kernel descreases. In  essence, the kernel is __flipped__ relative to the input. Flipping the kernel results in a commutative property of a convolution meaning we can equivalently write:\n",
    "\n",
    "$z_{i,j}=(\\mathbf W \\ast \\mathbf X)_{i,j}=\\sum_m\\sum_n x_{i-m,j-n} w_{m,n}$.\n",
    "\n",
    "Most neural network libraries implement a related function called the __cross-correlation__ (which is the same as convolution but without flipping the kernel) but refer to it as __convolution__.\n",
    "\n",
    "$z_{i,j}=(\\mathbf W \\ast \\mathbf X)_{i,j}=\\sum_m\\sum_n x_{i+m,j+n} w_{m,n}$.\n",
    "\n",
    "In machine learning, the output of a convolution is often referred to as a feature map.\n",
    "\n",
    "The convolution (cross-correlation) formulas as defined in this section can be generalized to tensors of an arbitrary rank. In deep learning applications, the most common are 1D, 2D, 3D, and 4D convolutions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variants of the basic convolution function\n",
    "\n",
    "In practical CNN application the input is usually a grid of vector-valued observations. E.g. RGB channels in a color image. Also we usually want to perform multiple convolutions in parallel. This is because a single kernel can extract only one kind of feature, albeit at many spatial locations. Usually we want each layer of the network to extract many kinds of features,at many locations. And finally, we almost always process inputs in a batch mode.\n",
    "\n",
    "As a result CNNs usually process multichannel convolutions. For example. Let input be RGB images encoded as rank 3 tensors (sometimes referred to as volumes) $\\mathbf X\\in \\mathbb R^{h \\times w \\times C_{in}}$ where $x_{i,j,k}$ is a pixel at row $i$, column $j$, and color channel $k$. If we want to convolve the input image with a set of kernels to generate $C_{out}$ feature maps the kernel tensor $\\mathbf W$ needs be rank 4 tensor $\\mathbf W\\in \\mathbf R^{k_h \\times k_w \\times C_{in} \\times C_{out}}$. The output volume of the multichannel convolution is defined as:\n",
    "\n",
    "$\\mathbf Z[:,:,l]=\\sum_{k=1}^{C_{in}}\\mathbf W[:,:,k,l] \\ast \\mathbf X[:,:,k]+\\mathbf b[l]\\quad$ where $l=\\{1,...,C_{out}\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
