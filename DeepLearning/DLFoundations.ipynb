{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Foundations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is deep learning?\n",
    "\n",
    "\n",
    "![Deep Learning](images/dl.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why deep representation?\n",
    "\n",
    "Circuit theory proves that there are functions that can be computed with \"narrow\" (relatively small number of hidden units in a layer) but deep (many layers) that shallower networks require exponentially more hidden units to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks as computational graphs\n",
    "\n",
    "![Computational graphs](images/graphs.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural network representation\n",
    "\n",
    "![Representation](images/representation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural network representation - vector notation\n",
    "\n",
    "![Representation](images/representation2.PNG)\n",
    "\n",
    "Let\n",
    "\n",
    "$\\mathbf X =\\begin{bmatrix} | & | &   & | \\\\ \n",
    "                \\mathbf x^{(1)} & \\mathbf x^{(2)} & \\cdots & \\mathbf x^{(m)} \\\\\n",
    "                          | & | &   & | \\end{bmatrix},\\qquad$\n",
    "$\\mathbf x^{(i)} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_{n^{[0]}}^{(i)} \\end{bmatrix},\\qquad$\n",
    "$\\mathbf X\\in\\mathbb R^{n^{[0]} \\times m},\\qquad$\n",
    "$\\mathbf x\\in\\mathbb R^{n^{[0]}}$\n",
    "\n",
    "$\\mathbf A^{[l]}=\\begin{bmatrix} | & | &   & | \\\\ \n",
    "                \\mathbf a^{[l](1)} & \\mathbf a^{[l](2)} & \\cdots & \\mathbf a^{[l](m)} \\\\\n",
    "                          | & | &   & | \\end{bmatrix},\\qquad$\n",
    "$\\mathbf a^{[l](i)} = \\begin{bmatrix} a_1^{[l](i)} \\\\ a_2^{[l](i)} \\\\ \\vdots \\\\ a_{n^{[l]}}^{[l](i)} \\end{bmatrix},\\qquad$       $\\mathbf A\\in\\mathbb R^{n^{[l]} \\times m},\\qquad$\n",
    "$\\mathbf a\\in\\mathbb R^{n^{[l]}}$           \n",
    "                          \n",
    "$\\mathbf W^{[l]}=\\begin{bmatrix} -\\mathbf w_{1}^{[l]}- \\\\ \n",
    "                                 -\\mathbf w_{2}^{[l]}- \\\\ \n",
    "                                 \\vdots \\\\ \n",
    "                                 -\\mathbf w_{n^{[l]}}^{[l]}- \\\\ \n",
    "                 \\end{bmatrix},\\qquad$\n",
    "$\\mathbf w_{i}^{[l]}=[w_1^{[l]},w_2^{[l]}, \\cdots, w_{n^{[l-1]}}^{[l]}],\\qquad$\n",
    "$\\mathbf b^{[l]}=\\begin{bmatrix} b_{1}^{[l]} \\\\ \n",
    "                                 b_{2}^{[l]} \\\\ \n",
    "                                 \\vdots \\\\ \n",
    "                                 b_{n^{[l]}}^{[l]} \\\\ \n",
    "                 \\end{bmatrix},\\qquad$ \n",
    "$\\mathbf W\\in\\mathbb R^{n^{[l]} \\times n^{[l-1]}},\\qquad$\n",
    "$\\mathbf b\\in\\mathbb R^{n^{[l]}}$   \n",
    "                          \n",
    "then               \n",
    "\n",
    "$\\mathbf Z^{[1]}=\\mathbf W^{[1]}\\mathbf X + \\mathbf b^{[1]},\\qquad$\n",
    "$\\mathbf A^{[1]}=\\sigma(\\mathbf Z^{[1]})$\n",
    "\n",
    "$\\mathbf Z^{[2]}=\\mathbf W^{[2]}\\mathbf A^{[1]} + \\mathbf b^{[2]},\\qquad$\n",
    "$\\mathbf A^{[2]}=\\sigma(\\mathbf Z^{[2]})$\n",
    "\n",
    "$\\mathbf Z^{[l]}=\\mathbf W^{[l]}\\mathbf A^{[l-1]} + \\mathbf b^{[l]},\\qquad$\n",
    "$\\mathbf A^{[l]}=\\sigma(\\mathbf Z^{[l]})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation functions\n",
    "\n",
    "\n",
    "![Activation functions](images/activations.PNG)\n",
    "\n",
    "$\\qquad\\qquad a=\\dfrac{1}{1+e^{-z}}\\qquad\\qquad\\qquad\\qquad\\qquad$\n",
    "$a=\\dfrac{e^z-e^{-z}}{e^z+e^{-z}}\\qquad\\qquad\\qquad\\qquad$\n",
    "$a=\\max(0,z)\\qquad\\qquad\\qquad\\qquad$\n",
    "$a=\\max(\\alpha z,z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output units\n",
    "\n",
    "Let's assume that the hidden layers compute a set of hidden features defined by\n",
    "\n",
    "$\\mathbf h=f(\\mathbf x; \\boldsymbol \\Theta),\\qquad$where $\\boldsymbol \\Theta$ is a set of all parameter tensors in hidden layers $\\boldsymbol \\Theta=\\{\\mathbf W^{[1]},...,\\mathbf W^{[L_h]},\\mathbf b^{[1]},...,\\mathbf b^{[L_h]}\\}$\n",
    "\n",
    "The role of the ouput layer is then to provide some additional transformation from the features to complete the task that the network must perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear units\n",
    "\n",
    "Given features $\\mathbf h$, a layer linear output units produces a vector\n",
    "\n",
    "$\\hat y=\\mathbf W^T\\mathbf h + \\mathbf b, \\quad$ where $\\mathbf W, \\mathbf b$ are parameter tensors of the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sigmoid units for binary classification tasks\n",
    "\n",
    "In the case of binary classification the output layer comprises a single unit.\n",
    "\n",
    "Given features $\\mathbf h$, a sigmoid output unit produces a scalar value that can be interpreted as a probability of a positive class. \n",
    "\n",
    "$\\hat y=\\sigma(\\mathbf w^T\\mathbf h + \\mathbf b), \\quad$ where $\\mathbf w, \\mathbf b$ are parameter vectors of the output unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Softmax units for multiclass classification tasks\n",
    "\n",
    "\n",
    "Given features $\\mathbf h$, a softmax output layer produces a vector $\\hat{\\mathbf y}$ that can be interpreted as the probablity distribution over $k$ different classes. \n",
    "\n",
    "Let\n",
    "\n",
    "$\\mathbf z = \\mathbf W^T\\mathbf h+\\mathbf b, \\quad$ where $\\mathbf W, \\mathbf b$ are parameter tensors of the output layer.\n",
    "\n",
    "$\\hat{y_i}=softmax(\\mathbf z)_i=\\dfrac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}, \\quad$ where $\\mathbf w, \\mathbf b$ are parameter vectors of the output unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other output types\n",
    "\n",
    "The linear, sigmoid, and softmax output units are the most common. However; neural networks can generalize to almost any kind of output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural network training\n",
    "\n",
    "![ANN training](images/training.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cost functions\n",
    "\n",
    "An important aspect of the design of a deep neural network is the choice of the cost function. The cost functions for neural networks are more or less the same as those for other parametric models.\n",
    "\n",
    "Most modern neural networks are trained using maximum likelihood. This meand that the cost function is simply the negative log-ikelihood, equivalenty described as the cross-entropy between the training data and the model distribution.\n",
    "\n",
    "$J(\\boldsymbol \\Theta)=-\\mathbb E_{\\mathbf x, \\mathbf y \\sim \\hat p_{data}} \\log p_{model}(\\mathbf y|\\mathbf x)$\n",
    "\n",
    "The specific form of the cost function depends on the form of $\\log p_{model}$ and as such on the type of units used in the output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-entropy cost for binary classification\n",
    "\n",
    "Let $\\mathcal D=\\{(\\mathbf x^{(1)},y^{(1)}),...,(\\mathbf x^{(m)},y^{(m)})\\}$ be a training data set and $\\hat y(\\mathbf x)$ the ouput of the network.\n",
    "\n",
    "\n",
    "$J(\\boldsymbol \\Theta)=-\\dfrac{1}{m}\\sum_{i=1}^m(y^{(i)} \\log(\\hat y^{(i)}) + (1-y^{(i)}) \\log (1-\\hat y^{(i)}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-entropy cost for multiclass classification\n",
    "\n",
    "Let $\\mathcal D=\\{(\\mathbf x^{(1)}, \\mathbf y^{(1)}),...,(\\mathbf x^{(m)}, \\mathbf y^{(m)})\\}$ be a training data set and $\\hat {\\mathbf y(\\mathbf x)}$ the ouput of the network.\n",
    "\n",
    "\n",
    "$J(\\boldsymbol \\Theta)=-\\dfrac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^C y_k^{(i)} \\log(\\hat y_k^{(i)}),\\quad $ where $C$ is the number of classes (the size of the output vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "To make it easier to understand, the following description shows how to calculate the gradient on a single training tuple $(\\mathbf x, \\mathbf y)$. In practice, backpropagation is vectorized and calculations are done on a whole minibatch. \n",
    "\n",
    "\n",
    "1.__Set the activations for the input layer__ $l=0$\n",
    "\n",
    "$\\qquad \\mathbf a^{[0]}=\\mathbf x$\n",
    "\n",
    "2.__Feedforward__: For each layer $l=1,2,...,L$: compute and cache:\n",
    "\n",
    "$\\qquad \\mathbf z^{[l]}=\\mathbf W^{[l]}\\mathbf a^{[l-1]}+\\mathbf b^{[l]},\\,$ and $\\mathbf a^{[l]}=\\sigma(\\mathbf z^{[l]})$\n",
    "\n",
    "3.__Compute the output error__ $\\boldsymbol \\delta^L$: \n",
    "\n",
    "$\\qquad \\boldsymbol \\delta^L = \\nabla_a J \\odot \\sigma^{'}(\\mathbf z^L)$\n",
    "\n",
    "4.__Backpropagate the error__: For each layer $l=L-1,L-2,...,1$ compute:\n",
    "\n",
    "$\\qquad \\boldsymbol \\delta^l = ((\\mathbf W^{l+1})^T \\boldsymbol \\delta^{l+1}) \\odot \\boldsymbol \\delta^{'} (\\mathbf z^l)$\n",
    "\n",
    "5.__Calculate gradient__:\n",
    "\n",
    "$\\qquad \\dfrac{\\partial J}{w_{ij}^l}=a_j^{[l-1]}\\delta_i^l\\quad$ and $\\dfrac{\\partial J}{b_{i}^l}=\\delta_i^l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivates of activaton functions\n",
    "\n",
    "__Sigmoid__\n",
    "\n",
    "\n",
    "\n",
    "$g(z)=\\dfrac{1}{1+e^{-z}} \\quad \\Longrightarrow \\quad \\dfrac{d}{dz}g(z)=g(z)(1-g(z))$\n",
    "\n",
    "$g(z)=\\dfrac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\quad \\Longrightarrow \\quad \\dfrac{d}{dz}g(z)=1-(g(z))^2$\n",
    "\n",
    "$g(z)=\\max(0,z) \\quad \\Longrightarrow \\quad \\dfrac{d}{dz}g(z)=\\left\\{ \\begin{array}{} 0\\, if\\, z \\lt 0 \\\\ 1\\, if\\, z \\geq 0 \\end{array}\\right., \\qquad$ Technically $\\dfrac{d}{dz}g(z)$ is not defined at 0 but in practice it can be \"overlooked\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
